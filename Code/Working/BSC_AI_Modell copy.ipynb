{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.losses import CategoricalCrossentropy \n",
    "import sys\n",
    "from keras import optimizers\n",
    "\n",
    "def column_one_hot (dataframe, columns): \n",
    "    for column in columns:\n",
    "        if column in dataframe:\n",
    "            one_hot = pd.get_dummies(dataframe[column])\n",
    "            dataframe = dataframe.drop(column,axis = 1)\n",
    "            dataframe = pd.concat([dataframe, one_hot], axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def drop_columns (dataframe, columns):\n",
    "    for column in columns:\n",
    "        if column in dataframe.columns:\n",
    "            dataframe = dataframe.drop(column, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def drop_column (dataframe, column):\n",
    "    if column in dataframe.columns:\n",
    "        dataframe = dataframe.drop(column, axis=1)\n",
    "    return dataframe\n",
    "\n",
    "def adjustShape(df_training, df_testing):\n",
    "    length = df_testing.shape[0]\n",
    "    df_temp = pd.concat([df_training, df_testing], ignore_index=True, sort=False)\n",
    "    df_temp = df_temp.fillna(0)\n",
    "    df_testing = df_temp.tail(length)\n",
    "    df_training = df_temp.drop(df_temp.tail(length).index)\n",
    "    return df_training, df_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SAR_Data.csv')\n",
    "\n",
    "status = df['Status'].unique()\n",
    "\n",
    "maskOpen = ((df['Status'] == 'postponed') | (df['Status'] == 'partly open') |( df['Status'] == 'In creation'))\n",
    "maskClosed = (df['Status'] == 'partly closed')\n",
    "df.loc[maskOpen, 'Status'] = 'open'\n",
    "df.loc[maskClosed, 'Status'] = 'closed'\n",
    "df.loc[(df['Status'] == 'non applicable'), 'Status'] = 'not applicable'\n",
    "status = df['Status'].unique()\n",
    "\n",
    "# postponed, partly open, in creation -> open;    partly closed -> closed\n",
    "\n",
    "# falsche Ordnerstruktur in /RA Application Conditions/03_PG_OCS/Service and diagnostic systems\n",
    "# fehlt ein Ordner bevor Module kommen, deshalb händisch eintragen\n",
    "\n",
    "df.loc[df['Version'].str.contains('VICOS_S_D'), 'Product'] = 'VICOS_S_D'\n",
    "df.loc[df['Version'].str.contains('VICOS_S_D'), 'Version'] = df['Version'].str[-5:]\n",
    "\n",
    "paths = df['Path'].unique()\n",
    "accessDB = pd.read_xml(\"X:/File/DE/bwga024a_IMORA_RM/05_Process_Management/14_Metriken & KPI/KPI-Erhebung/KPI_01-04_General/Data/Input/Input_BWG_Combined_Access.xml\")\n",
    "\n",
    "for path in paths:\n",
    "    try:\n",
    "        if(path == \"/ML Realization Projects Algeria\"):\n",
    "            result = accessDB.loc[(accessDB['Type'] == \"Real\") & (accessDB['Location'] == \"BWG\") & (accessDB['Offset'] == \"/ML Realization Projects Algeria/20006_ML_BM_Boughezoul_MSila\")].iloc[0]\n",
    "        else:\n",
    "            result = accessDB.loc[(accessDB['Type'] == \"Real\") & (accessDB['Location'] == \"BWG\") & ((accessDB['Offset'] == str(path)) | (accessDB['Offset'] == (str(path) + \"/\")))].iloc[0]           \n",
    "    except:\n",
    "        print(str(path) + \" has no entry in the AccessDB!\")\n",
    "\n",
    "    mask = df['Path'] == str(path)\n",
    "    df.loc[mask, 'Project_category'] = result['Project_category']\n",
    "    df.loc[mask, 'BS'] = result['BS']\n",
    "    df.loc[mask, 'RU'] = result['RU']\n",
    "    df.loc[mask, 'ProjectYear'] = result['ProjectYear']\n",
    "    df.loc[mask, 'section'] = result['section']\n",
    "    df.loc[mask, 'Project_name'] = result['Project_name']\n",
    "    df['ProductVersion'] = df[\"Product\"].str.cat(df[\"Version\"], sep = \"-\")\n",
    "\n",
    "df['ProjectYear'] = df['ProjectYear'].astype('int')\n",
    "df = df[['Text', 'Product', 'ProductVersion', 'Project_name', 'section', 'Project_category', 'BS', 'RU', 'ProjectYear', 'Status', 'Statement']]\n",
    "#df = drop_column(df, 'Statement') # später wieder löschen!\n",
    "\n",
    "\n",
    "# TEST (WORKING!) ---------------------------------------\n",
    "products = df['Product'].unique()\n",
    "df = column_one_hot(df, ['Product'])\n",
    "projects = df['Project_name'].unique()\n",
    "for project in projects:\n",
    "    for product in products:\n",
    "        df.loc[df['Project_name'] == project, product] = 1 if (df.loc[df['Project_name'] == project][product].sum()) >= 1 else 0\n",
    "df = column_one_hot(df, ['ProductVersion', 'Project_name', 'section',\n",
    "       'Project_category', 'BS', 'RU', 'ProjectYear'])\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df\n",
    "text = \"Zur Anschaltung des Antriebes in der Außenanlage müssen Signalkabel nach VDE 0816/2 oder Kabel mit vergleichbaren Eigenschaften verwendet werden. Die Verlegevorschriften des Kabels sind einzuhalten.\"\n",
    "df_training = df_training.loc[df_training['Text'] == text].reset_index(drop=True)\n",
    "\n",
    "#test_col = 8\n",
    "#test = drop_columns(df_training.iloc[[test_col]], ['Text', 'Status', 'Statement'])\n",
    "#df_training = df_training.drop(test_col)\n",
    "\n",
    "df_training = df_training.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "trainX = drop_columns(df_training, ['Status', 'Text', 'Statement'])\n",
    "trainYStatus = drop_column(column_one_hot(df_training[['Text', 'Status']], ['Status']), \"Text\")\n",
    "trainYStatement = drop_column(column_one_hot(df_training[['Text', 'Statement']], ['Statement']), \"Text\")\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functional\n",
    "inputs = Input(shape=(trainX.shape[1],))\n",
    "x = Dense(32, activation='relu')(inputs)\n",
    "x = Dense(16, activation='relu')(x)\n",
    "output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                    batch_size=1,\n",
    "                    epochs=50,\n",
    "                    verbose=0,\n",
    "                    validation_split=0.4)\n",
    "\n",
    "prediction_status, prediction_statement = model.predict(test)\n",
    "\n",
    "for val in prediction_status:\n",
    "    for col in range(len(val)):\n",
    "        print (str(trainYStatus.columns[col]) + \" \" + str(val[col]))\n",
    "\n",
    "#predictionStatement = modelStatement.predict(test)\n",
    "#index_max = np.argmax(predictionStatement)\n",
    "#print (trainYStatement.columns[index_max] + \" \" + '{:.1%}'.format(predictionStatement[0][index_max]))\n",
    "#col = 0\n",
    "#for i in predictionStatement:\n",
    "#    for j in i:\n",
    "#        print (trainYStatement.columns[col] + \" \" + '{:.1%}'.format(j))\n",
    "#        col += 1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests below---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adam = []\n",
    "all_rmsprop = []\n",
    "all_sgd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adam = []\n",
    "all_adam_small = []\n",
    "all_adam_large = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-cross Validierung\n",
    "df_training = df\n",
    "text = \"Zur Anschaltung des Antriebes in der Außenanlage müssen Signalkabel nach VDE 0816/2 oder Kabel mit vergleichbaren Eigenschaften verwendet werden. Die Verlegevorschriften des Kabels sind einzuhalten.\"\n",
    "df_training = df_training.loc[df_training['Text'] == text]\n",
    "df_training = df_training.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#all_val_loss = []\n",
    "#all_loss     = []\n",
    "#all_acc     = []\n",
    "#all_val_acc = []\n",
    "\n",
    "num_epochs = 250\n",
    "k = 4\n",
    "for i in range(k):\n",
    "    start = int(i/k * len(df_training))\n",
    "    end = int((i/k + 0.25) * len(df_training))\n",
    "    val_data = df_training.iloc[start:end] \n",
    "    train_data = df_training.drop(range(start,end))\n",
    "\n",
    "    trainX = drop_columns(train_data, ['Status', 'Text', 'Statement'])\n",
    "    trainYStatus = drop_column(column_one_hot(train_data[['Text', 'Status']], ['Status']), \"Text\")\n",
    "    trainYStatement = drop_column(column_one_hot(train_data[['Text', 'Statement']], ['Statement']), \"Text\")\n",
    "\n",
    "    valX = drop_columns(val_data, ['Status', 'Text', 'Statement'])\n",
    "    trainX, valX = adjustShape(trainX, valX)\n",
    "\n",
    "    valYStatus = drop_column(column_one_hot(val_data[['Text', 'Status']], ['Status']), \"Text\")\n",
    "    trainYStatus, valYStatus = adjustShape(trainYStatus, valYStatus)\n",
    "\n",
    "    valYStatement = drop_column(column_one_hot(val_data[['Text', 'Statement']], ['Statement']), \"Text\")\n",
    "    trainYStatement, valYStatement = adjustShape(trainYStatement, valYStatement)\n",
    "\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(8, activation='relu')(inputs)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    all_adam_small.append(history.history['val_status_loss'])\n",
    "    #all_val_loss.append(history.history['val_status_loss'])\n",
    "    #all_loss.append(history.history['status_loss'])\n",
    "    #all_acc.append(history.history['status_categorical_accuracy'])\n",
    "    #all_val_acc.append(history.history['val_status_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-cross-Validierung\n",
    "adam        = [np.mean([x[i] for x in all_adam])\n",
    "                    for i in range(num_epochs-1)]\n",
    "rmsprop     = [np.mean([x[i] for x in all_rmsprop])\n",
    "                    for i in range(num_epochs-1)]\n",
    "sgd         = [np.mean([x[i] for x in all_sgd])\n",
    "                    for i in range(num_epochs-1)]\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(adam) + 1), adam, label = 'Adam-Optimierer')\n",
    "plt.plot(range(1, len(rmsprop) + 1), rmsprop, label = 'RMSprop-Optimierer')\n",
    "plt.plot(range(1, len(sgd) + 1), sgd, label = 'SGD-Optimierer')\n",
    "plt.title('Mittlerer Wert der Verlustfunktion')\n",
    "plt.xlabel('Epochen')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-cross-Validierung\n",
    "adam        = [np.mean([x[i] for x in all_adam])\n",
    "                    for i in range(num_epochs-1)]\n",
    "adamS     = [np.mean([x[i] for x in all_adam_small])\n",
    "                    for i in range(num_epochs-1)]\n",
    "adamL         = [np.mean([x[i] for x in all_adam_large])\n",
    "                    for i in range(num_epochs-1)]\n",
    "\n",
    "\n",
    "plt.plot(range(1, len(adam) + 1), adam, label = 'Lr = 0,001')\n",
    "plt.plot(range(1, len(adamS) + 1), adamS, label = 'Lr = 0,0001')\n",
    "plt.plot(range(1, len(adamL) + 1), adamL, label = 'Lr = 0,01')\n",
    "plt.title('Mittlerer Wert der Verlustfunktion')\n",
    "plt.xlabel('Epochen')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test verschiedener Architekturen\n",
    "m1_all = []\n",
    "m2_all = []\n",
    "m3_all = []\n",
    "m4_all = []\n",
    "m5_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df\n",
    "text = \"Zur Anschaltung des Antriebes in der Außenanlage müssen Signalkabel nach VDE 0816/2 oder Kabel mit vergleichbaren Eigenschaften verwendet werden. Die Verlegevorschriften des Kabels sind einzuhalten.\"\n",
    "df_training = df_training.loc[df_training['Text'] == text]\n",
    "df_training = df_training.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "num_epochs = 250\n",
    "k = 4\n",
    "for i in range(k):\n",
    "    start = int(i/k * len(df_training))\n",
    "    end = int((i/k + 0.25) * len(df_training))\n",
    "    val_data = df_training.iloc[start:end] \n",
    "    train_data = df_training.drop(range(start,end))\n",
    "\n",
    "    trainX = drop_columns(train_data, ['Status', 'Text', 'Statement'])\n",
    "    trainYStatus = drop_column(column_one_hot(train_data[['Text', 'Status']], ['Status']), \"Text\")\n",
    "    trainYStatement = drop_column(column_one_hot(train_data[['Text', 'Statement']], ['Statement']), \"Text\")\n",
    "\n",
    "    valX = drop_columns(val_data, ['Status', 'Text', 'Statement'])\n",
    "    trainX, valX = adjustShape(trainX, valX)\n",
    "\n",
    "    valYStatus = drop_column(column_one_hot(val_data[['Text', 'Status']], ['Status']), \"Text\")\n",
    "    trainYStatus, valYStatus = adjustShape(trainYStatus, valYStatus)\n",
    "\n",
    "    valYStatement = drop_column(column_one_hot(val_data[['Text', 'Statement']], ['Statement']), \"Text\")\n",
    "    trainYStatement, valYStatement = adjustShape(trainYStatement, valYStatement)\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(32, activation='relu')(inputs)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    m1_all.append(history.history['val_status_loss'])\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    m2_all.append(history.history['val_status_loss'])\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(256, activation='relu')(inputs)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    m3_all.append(history.history['val_status_loss'])\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(16, activation='relu')(inputs)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    m4_all.append(history.history['val_status_loss'])\n",
    "# -----------------------------------------------------------------------------------------------------------------\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    x = Dense(512, activation='relu')(inputs)\n",
    "    output1 = Dense(trainYStatus.shape[1], activation='softmax', name='status')(x)\n",
    "    output2 = Dense(trainYStatement.shape[1], activation='softmax', name='statement')(x)\n",
    "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "                    loss={'status': CategoricalCrossentropy(), 'statement': CategoricalCrossentropy()},\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "    history = model.fit(trainX, {'status': trainYStatus, 'statement': trainYStatement},\n",
    "                        batch_size=1,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=2,\n",
    "                        validation_data = (valX, {'status': valYStatus, 'statement': valYStatement}))\n",
    "    m5_all.append(history.history['val_status_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-cross-Validierung\n",
    "m1          = [np.mean([x[i] for x in m1_all])\n",
    "                    for i in range(num_epochs-1)]\n",
    "m2          = [np.mean([x[i] for x in m2_all])\n",
    "                    for i in range(num_epochs-1)]\n",
    "m3          = [np.mean([x[i] for x in m3_all])\n",
    "                    for i in range(num_epochs-1)]\n",
    "m4          = [np.mean([x[i] for x in m4_all])\n",
    "                    for i in range(num_epochs-1)]\n",
    "m5          = [np.mean([x[i] for x in m5_all])\n",
    "                    for i in range(num_epochs-1)]\n",
    "\n",
    "plt.plot(range(1, len(m1) + 1), m1, label = 'M1')\n",
    "plt.plot(range(1, len(m2) + 1), m2, label = 'M2')\n",
    "plt.plot(range(1, len(m3) + 1), m3, label = 'M3')\n",
    "plt.plot(range(1, len(m4) + 1), m4, label = 'M4')\n",
    "plt.plot(range(1, len(m5) + 1), m5, label = 'M5')\n",
    "plt.title('Mittlerer Wert der Verlustfunktion')\n",
    "plt.xlabel('Epochen')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1Test = m1\n",
    "index_min = np.argmin(m1Test)\n",
    "m1Test[index_min]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
