\section{Definieren des neuronalen Netzes}
Nachdem der Trainingsdatensatz definiert und in Ein- und Ausgabewerte aufgeteilt wurde, muss als Nächstes das \ac{NN} definiert werden, wie in Kapitel \ref{chap:Keras} beschrieben wurde.
Dafür bietet die Keras-Bibliothek die Klasse \glqq Sequential\grqq{}, die genutzt werden kann, wenn Schichten sequentiell angeordnet werden sollen. Laut Chollet ist diese Art der 
Netzarchitektur die am häufigsten genutzte \cite[vgl. S.92]{DL_PY}. 

\begin{lstlisting}[language = python, caption={Erstellung eines sequentiellen Modells},captionpos=b, label = lst:ModellSeq, floatplacement=H]
    model = Sequential()
    model.add(Dense(8, input_shape=(trainX.shape[1],), activation='relu'))
    model.add(Dense(16, activation='relu'))
    model.add(Dense(trainYStatus.shape[1], activation='softmax'))
\end{lstlisting}

Der Quellcode \ref{lst:ModellSeq} zeigt, wie ein Modell zur Vorhersage des Statuswerts definiert werden kann. Zuerst wird ein Objekt vom Typ Sequential erstellt. Diesem Objekt 
können dann die verschiedenen Schichten hinzugefügt werden. Dabei muss bestimmt werden, wie viele Schichten das Modell haben soll und wie viele Neuronen jede Schicht besitzen soll.
Als Erste Schicht wird eine Dense-Layer als Eingabeschicht mit acht Neuronen definiert. Eine Dense-Layer ist eine fully-connected Schicht,
wie beispielsweise die Schichten in der Abbildung \ref{fig:NN_Modell}, und erwartet als Parameter mindestens die Anzahl an Neuronen, optional können noch weitere Parameter angegeben werden.
Die Eingabeschicht benötigt als Besonderheit noch zusätzlich die Anzahl an Attributen, diese können dem Dataframe mit den Eingabewerten entnommen werden. Als Aktivierungsfunktion wird
die \ac{ReLU}-Funktion genutzt, die bereits in Kapitel \ref{chap:DL} vorgestellt wurde und auch die verbreitetste Aktivierungsfunktion beim \ac{DL} ist \cite[vgl. S.102]{DL_PY}. 
Als erste versteckte Schicht wird eine weitere Dense-Layer genutzt, hier mit 16 Neuronen. 
Die Anzahl an Neuronen und die Tiefe des Modells bestimmt die Komplexität des Modells. Ein zu komplexes Modell kann zu Überanpassung und ein zu simples Modell zu Unteranpassung führen.
Im Vorhinein ist es schwierig die benötigte Komplexität zu bestimmen, weshalb die Anzahl an Schichten und Neuronen, in Abhängigkeit mit dem Ergebnis des Modells, im Laufe der Erstellung noch geändert 
werden sollten. Als letzte Schicht und somit als Ausgabeschicht wird ebenfalls eine Dense-Layer verwendet. Die Anzahl an Neuronen in der Ausgabeschicht ist abhängig von der Anzahl an 
möglichen Ausprägungen des zu bestimmenden Wertes. Die ausgewählte Anwendungsregel wurde in der Vergangenheit mit \glqq closed\grqq{}, \glqq compliant\grqq{} und \glqq not applicable\grqq{} bewertet,
weshalb in diesem Fall die Ausgabeschicht drei Neuronen besitzen muss. Jedes Neuron stellt dabei eine mögliche Ausprägung dar. Anders als bei den vorherigen Schichten wurde bei dieser Schicht 
als Aktivierungsfunktion \glqq softmax\grqq{} gewählt. Diese Aktivierungsfunktion weist jedem Neuron eine Wahrscheinlichkeit zwischen null und eins zu, wobei die Summe aller Wahrscheinlichkeiten 
eins ergibt \cite{KerasDoc}. Die Auswahl der Aktivierungsfunktion der Ausgabeschicht ist abhängig von der Aufgabe des Modells. \glqq Softmax\grqq{} bietet sich zum Beispiel ideal als Möglichkeit zur Klassifikation an,
wenn mehrere verschiedene Klassifizierungen vorhanden sind. Wenn nur zwei verschiedene Zustände möglich sind, also ein binäres Klassifikationsproblem vorliegt, würde sich als Aktivierungsfunktion
eine Sigmoidfunktion anbieten, da diese beliebige Werte einen Ausgabebereich zwischen null und eins zuordnet \cite[vgl. S.100]{DL_PY}.
Eine Zusammenfassung wurde mit der summary()-Methode im Quellcode \ref{lst:ModellSummary} ausgegeben.
\begin{lstlisting}[language = python, caption={Zusammenfassung des Modells},captionpos=b, label = lst:ModellSummary, float, floatplacement=H]
    model.summary()
    ---------------------------------------
    Output:
    Model: "sequential"
    _______________________________________________________________
    Layer (type)                Output Shape              Param #   
    ===============================================================
    dense (Dense)               (None, 8)                 1424      
                                                                    
    dense_1 (Dense)             (None, 16)                144       
                                                                    
    dense_2 (Dense)             (None, 3)                 51        
                                                                    
    ===============================================================
    Total params: 1,619
    Trainable params: 1,619
    Non-trainable params: 0
    _______________________________________________________________
\end{lstlisting}
Diese Zusammenfassung zeigt nochmal die verschiedenen Schichten, die Dimension der Ausgabe der Schichten sowie die Anzahl an Parametern in jeder Schicht. 
Zu erkennen sind dort die drei hinzugefügten Dense-Layer. Die erste Dimension der Ausgabe ist \glqq None\grqq{}, da die Anzahl an Einträgen vorher nicht festgelegt wurde. 
Die zweite Dimension wird bestimmt durch die Anzahl an Neuronen in einer Schicht.\\

Je nach Anwendungsfall würden andere Schichten als die Dense-Layer infrage kommen. Stehen die Daten in einem sequentiellen Zusammenhang, beispielsweise eine Zeitreihe von Wetterdaten, dann würden
die sogenannten LSTM-Layer infrage kommen. LSTM steht für Long Short-Term-Memory(auf Deutsch: langes Kurzzeitgedächtnis) und diese Art von Schicht ist in der Lage Informationen mehrere Zeitschritte
lang zu erhalten \cite[vgl. S.260]{DL_PY}. Bei der Computer Vision werden häufig CNNs, also Convolutional Neuronal Networks, genutzt. Ein CNN besteht dabei nicht aus Dense-Layern, wie in diesem 
Anwendungsfall, sondern aus Convolutional-Layern. Diese Schichten können zum Beispiel lokale Muster in Bildern erlernen und diese in neuen Bildern wiedererkennen, auch wenn sie nicht an derselben Stelle
sind \cite[vgl. S.164]{DL_PY}. Diese Schichten werden also in anderen Anwendungsfällen genutzt, für diesen Anwendungsfall eignen sich jedoch die Dense-Layer am besten.    \\

Bevor das Modell mit den Daten angelernt werden kann, müssen zunächst noch die Verlustfunktion und ein Optimierer ausgewählt werden. Da der Aufgabentyp eine Single-Lable-Mehrfachklassifizierung 
ist, also eine Klassifizierung wo eine Klasse aus mehreren Klassen gewählt werden muss, wird als Verlustfunktion die kategorische Kreuzentropie genutzt.
Bei der kategorischen Kreuzentropie wird erwartet, dass die Ausgabewerte in der One-Hot-Codierung vorliegen. Sind die Ausgabewerte mit dem Label-Encoding codiert,
müsste als Verlustfunktion die \glqq Sparse Categorical Crossentropy\grqq{} genutzt werden \cite{KerasDoc}. Für andere Aufgabentypen werden andere 
Verlustfunktionen genutzt. Zum Beispiel bei einer Regression bietet sich der mittlere quadratische Fehler an oder bei der Binärklassifizierung die binäre Kreuzentropie \cite[vgl. S.155]{DL_PY}.
Während des Trainings wird das Modell versuchen das Ergebnis der kategorischen Kreuzentropie zu minimieren. 

